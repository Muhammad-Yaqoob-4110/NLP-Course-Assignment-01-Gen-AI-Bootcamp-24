{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Text Collection and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fantastic spot for an even or a quite cocktail...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love, love, love the calamari. It's so good an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Love this place. Stiff martinis and cocktails,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's everything a great cocktail bar should be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I came here before a pirates game, so it was a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  class\n",
       "0  Fantastic spot for an even or a quite cocktail...      1\n",
       "1  Love, love, love the calamari. It's so good an...      1\n",
       "2  Love this place. Stiff martinis and cocktails,...      1\n",
       "3  It's everything a great cocktail bar should be...      1\n",
       "4  I came here before a pirates game, so it was a...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I have collected and load a dataset from a kaggle containing ecommerce-reviews for processing.\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"TestReviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import natual language toolkit\n",
    "import nltk\n",
    "\n",
    "# The Gutenberg Corpus is a collection of texts compiled by Project Gutenberg1. Project Gutenberg is an organization that aims to digitize and make available public domain books. The corpus contains over 53,000 books in English, German, French, Spanish, and other languages\n",
    "\n",
    "# Load the Gutenberg corpus\n",
    "corpus = nltk.corpus.gutenberg.raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: Split the text into words and sentences. I have splited Gutenberg Corpus into both words and sentences for better understanding \n",
    "words = nltk.tokenize.word_tokenize(corpus)\n",
    "sentences = nltk.tokenize.sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']\n",
      "['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', \"She was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\"]\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming: Reduce words to their root form using Porter Stemmer. Basically, Stemming is a hard-cut. It cut the words from the end and does not look at the stemed words wheither they have context or not.\n",
    "# In other words, Stemming is crude heuristic that chop off the words from the end in the hope of achieving the goal correctly.\n",
    "#  The resulting stem may not always be a valid word. It can produce invalid words.\n",
    "\n",
    "# i have used a porter's algorithm for stemming \n",
    "porter = nltk.stem.PorterStemmer()\n",
    "stemmed_words = [porter.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_words[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization: Reduce the words by considering their context\n",
    "# In other words, Lemmatization uses a vocabulary and morphological analysis of words to return the base or dictionary form of a word which is known as the lemma.\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessing', 'of', 'existence', ';']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_words[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Word Removal: Eliminate common words that may not be useful for analysis\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'CHAPTER', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'rich', ',', 'comfortable', 'home', 'happy', 'disposition', ',', 'seemed', 'unite', 'best', 'blessing', 'existence', ';', 'lived', 'nearly', 'twenty-one', 'year', 'world', 'little', 'distress', 'vex', '.', 'wa', 'youngest', 'two']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_words[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Feature Extraction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words(BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  document  first  is  one  second  the  third  this\n",
      "0    0         1      1   1    0       0    1      0     1\n",
      "1    0         2      0   1    0       1    1      0     1\n",
      "2    1         0      0   1    1       0    1      1     1\n",
      "3    0         1      1   1    0       0    1      0     1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# In bag of words, we convert the text to numeric data. Here is the process for text to bag of words conversion\n",
    "# Step 1: First we make list of unique words from all documents\n",
    "# Step 2: Create a vectors for the document\n",
    "\n",
    "# Given corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert bow_matrix to a DataFrame\n",
    "bow_df = pd.DataFrame.sparse.from_spmatrix(bow_matrix, columns=feature_names)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(bow_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and  document     first        is       one    second       the  \\\n",
      "0         0  0.469791  0.580286  0.384085         0         0  0.384085   \n",
      "1         0  0.687624         0  0.281089         0  0.538648  0.281089   \n",
      "2  0.511849         0         0  0.267104  0.511849         0  0.267104   \n",
      "3         0  0.469791  0.580286  0.384085         0         0  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0         0  0.384085  \n",
      "1         0  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3         0  0.384085  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF stands for term frequency - inverse document frequency. \n",
    "# TF-IDF is used to evaluate how important a word is to a document in corpus.\n",
    "# Term Frequency is calculated as No. of term appears in a document/Total no. of terms in the document\n",
    "# Inverse Document Frequency IDF is calculated as log(total no. of documents/ no. of documents with term in it.)\n",
    "# Finally, TF-IDF will be TF * IDF\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names_tfidf = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "# Convert tfidf_matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=feature_names_tfidf)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(tfidf_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
